{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This doc is trying to find out why 768 is used and important in nlp transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 2\n",
      "192 2\n",
      "96 2\n",
      "48 2\n",
      "24 2\n",
      "12 2\n",
      "6 2\n",
      "3 2\n",
      "2^8 * 3^1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def is_prime(n):\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def sqrt(n: int):\n",
    "    return int(n**0.5) + 1\n",
    "\n",
    "\n",
    "def factorise(n: int):\n",
    "    factors = []\n",
    "    i = 2\n",
    "    while n > 1:\n",
    "        for i in range(2, sqrt(n)):\n",
    "            if n % i == 0:\n",
    "                factors.append(i)\n",
    "                n = n // i\n",
    "                print(n, i)\n",
    "                break\n",
    "\n",
    "        if is_prime(n):\n",
    "            factors.append(n)\n",
    "            break\n",
    "\n",
    "    f = set(factors)\n",
    "\n",
    "    return factors , { i: factors.count(i) for i in f }\n",
    "\n",
    "\n",
    "\n",
    "f, exponents = factorise(768)\n",
    "print( ' * '.join(f'{i}^{exp}' for i, exp in exponents.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BERT_base\n",
    "\n",
    "### Layers\n",
    "Sequential([\n",
    "12 Encoders blocks\n",
    "Linear layer only for the first token [CLS] (1,hidden)\n",
    "Linear (hidden, vocab_size)\n",
    "Softmax layer\n",
    "])\n",
    "Encoder block:\n",
    "Sequential([\n",
    "    MultiHeadAttention( 12 heads, 768 dim)\n",
    "    Add & Norm\n",
    "    ProjectionLayer( 768 , 3072 )\n",
    "    Activation\n",
    "    ProjectionLayer( 3072 , 768 )\n",
    "    Add & Norm\n",
    "])\n",
    "## Explaination\n",
    "\n",
    "The token are prepended with a special token [CLS] and appended with a special token [SEP]\n",
    "The token are then passed through the embedding layer and the positional encoding layer\n",
    "The output of the positional encoding layer is passed through the 12 encoder blocks\n",
    "So the batch shape is (batch_size, seq_len, 768)\n",
    "The output of the last encoder block is passed through a linear layer to get the logits\n",
    "Note that 768 is the hidden size of the model and the size of the embedding token\n",
    "## Training\n",
    "The output of the last encoder block is clipped to the first token [CLS]\n",
    "Vocab size is the number of classes\n",
    "The encoder block are trained in pretraining, the linear layer is trained in fine-tuning by prepending the sequences with a  [CLS] and training the model to predict the class of the sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
