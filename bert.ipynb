{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This doc is trying to find out why 768 is used and important in nlp transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ --------------------\n",
      "anyio                    3.7.0\n",
      "argon2-cffi              21.3.0\n",
      "argon2-cffi-bindings     21.2.0\n",
      "arrow                    1.2.3\n",
      "asttokens                2.2.1\n",
      "async-lru                2.0.2\n",
      "attrs                    23.1.0\n",
      "Babel                    2.12.1\n",
      "backcall                 0.2.0\n",
      "beautifulsoup4           4.12.2\n",
      "bleach                   6.0.0\n",
      "certifi                  2019.11.28\n",
      "cffi                     1.15.1\n",
      "chardet                  3.0.4\n",
      "charset-normalizer       3.1.0\n",
      "comm                     0.1.3\n",
      "dbus-python              1.2.16\n",
      "debugpy                  1.6.7\n",
      "decorator                5.1.1\n",
      "defusedxml               0.7.1\n",
      "exceptiongroup           1.1.1\n",
      "executing                1.2.0\n",
      "fastjsonschema           2.17.1\n",
      "fqdn                     1.5.1\n",
      "idna                     2.8\n",
      "ipykernel                6.23.3\n",
      "ipython                  8.14.0\n",
      "isoduration              20.11.0\n",
      "jedi                     0.18.2\n",
      "Jinja2                   3.1.2\n",
      "json5                    0.9.14\n",
      "jsonpointer              2.4\n",
      "jsonschema               4.17.3\n",
      "jupyter_client           8.3.0\n",
      "jupyter_core             5.3.1\n",
      "jupyter-events           0.6.3\n",
      "jupyter-lsp              2.2.0\n",
      "jupyter_server           2.7.0\n",
      "jupyter_server_terminals 0.4.4\n",
      "jupyterlab               4.0.2\n",
      "jupyterlab-pygments      0.2.2\n",
      "jupyterlab_server        2.23.0\n",
      "MarkupSafe               2.1.3\n",
      "matplotlib-inline        0.1.6\n",
      "mistune                  3.0.1\n",
      "nbclient                 0.8.0\n",
      "nbconvert                7.6.0\n",
      "nbformat                 5.9.0\n",
      "nest-asyncio             1.5.6\n",
      "notebook_shim            0.2.3\n",
      "overrides                7.3.1\n",
      "packaging                23.1\n",
      "pandocfilters            1.5.0\n",
      "parso                    0.8.3\n",
      "pexpect                  4.8.0\n",
      "pickleshare              0.7.5\n",
      "pip                      23.1.2\n",
      "platformdirs             3.8.0\n",
      "prometheus-client        0.17.0\n",
      "prompt-toolkit           3.0.38\n",
      "psutil                   5.9.5\n",
      "ptyprocess               0.7.0\n",
      "pure-eval                0.2.2\n",
      "pycparser                2.21\n",
      "Pygments                 2.15.1\n",
      "PyGObject                3.36.0\n",
      "pyrsistent               0.19.3\n",
      "python-apt               2.0.1+ubuntu0.20.4.1\n",
      "python-dateutil          2.8.2\n",
      "python-json-logger       2.0.7\n",
      "PyYAML                   6.0\n",
      "pyzmq                    25.1.0\n",
      "requests                 2.31.0\n",
      "requests-unixsocket      0.2.0\n",
      "rfc3339-validator        0.1.4\n",
      "rfc3986-validator        0.1.1\n",
      "Send2Trash               1.8.2\n",
      "setuptools               68.0.0\n",
      "six                      1.14.0\n",
      "sniffio                  1.3.0\n",
      "soupsieve                2.4.1\n",
      "stack-data               0.6.2\n",
      "terminado                0.17.1\n",
      "tinycss2                 1.2.1\n",
      "tomli                    2.0.1\n",
      "tornado                  6.3.2\n",
      "traitlets                5.9.0\n",
      "typing_extensions        4.6.3\n",
      "uri-template             1.3.0\n",
      "urllib3                  1.25.8\n",
      "wcwidth                  0.2.6\n",
      "webcolors                1.13\n",
      "webencodings             0.5.1\n",
      "websocket-client         1.6.1\n",
      "wheel                    0.40.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGC-DL-CONTAINER-LICENSE  home\t       media\t      run\tusr\n",
      "bin\t\t\t  jupyter.log  mnt\t      sbin\tvar\n",
      "boot\t\t\t  lib\t       opt\t      srv\tworkspace\n",
      "dev\t\t\t  lib32        post_start.sh  start.sh\n",
      "etc\t\t\t  lib64        proc\t      sys\n",
      "get-pip.py\t\t  libx32       root\t      tmp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 2\n",
      "192 2\n",
      "96 2\n",
      "48 2\n",
      "24 2\n",
      "12 2\n",
      "6 2\n",
      "3 2\n",
      "2^8 * 3^1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def is_prime(n):\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def sqrt(n: int):\n",
    "    return int(n**0.5) + 1\n",
    "\n",
    "\n",
    "def factorise(n: int):\n",
    "    factors = []\n",
    "    i = 2\n",
    "    while n > 1:\n",
    "        for i in range(2, sqrt(n)):\n",
    "            if n % i == 0:\n",
    "                factors.append(i)\n",
    "                n = n // i\n",
    "                print(n, i)\n",
    "                break\n",
    "\n",
    "        if is_prime(n):\n",
    "            factors.append(n)\n",
    "            break\n",
    "\n",
    "    f = set(factors)\n",
    "\n",
    "    return factors , { i: factors.count(i) for i in f }\n",
    "\n",
    "\n",
    "\n",
    "f, exponents = factorise(768)\n",
    "print( ' * '.join(f'{i}^{exp}' for i, exp in exponents.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BERT_base\n",
    "\n",
    "### Layers\n",
    "Sequential([\n",
    "12 Encoders blocks\n",
    "Linear layer only for the first token [CLS] (1,hidden)\n",
    "Linear (hidden, vocab_size)\n",
    "Softmax layer\n",
    "])\n",
    "Encoder block:\n",
    "Sequential([\n",
    "    MultiHeadAttention( 12 heads, 768 dim)\n",
    "    Add & Norm\n",
    "    ProjectionLayer( 768 , 3072 )\n",
    "    Activation\n",
    "    ProjectionLayer( 3072 , 768 )\n",
    "    Add & Norm\n",
    "])\n",
    "## Explaination\n",
    "\n",
    "The token are prepended with a special token [CLS] and appended with a special token [SEP]\n",
    "The token are then passed through the embedding layer and the positional encoding layer\n",
    "The output of the positional encoding layer is passed through the 12 encoder blocks\n",
    "So the batch shape is (batch_size, seq_len, 768)\n",
    "The output of the last encoder block is passed through a linear layer to get the logits\n",
    "Note that 768 is the hidden size of the model and the size of the embedding token\n",
    "## Training\n",
    "The output of the last encoder block is clipped to the first token [CLS]\n",
    "Vocab size is the number of classes\n",
    "The encoder block are trained in pretraining, the linear layer is trained in fine-tuning by prepending the sequences with a  [CLS] and training the model to predict the class of the sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path: bert.ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERTBlock(nn.Module):\n",
    "\n",
    "    hidden_layer = 768\n",
    "    def __init__(self, token_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention( token_size, num_heads=8, batch_first=True)\n",
    "        self.qkv = nn.Linear(token_size, self.hidden_layer * 3)\n",
    "        # self.Q = nn.Linear(input_size, self.hidden_size)\n",
    "        # self.K = nn.Linear(input_size, self.hidden_size)\n",
    "        # self.V = nn.Linear(input_size, self.hidden_size)\n",
    "\n",
    "        self.proj_h = nn.Linear(self.hidden_layer, self.hidden_layer*4)\n",
    "        self.proj_out = nn.Linear(self.hidden_layer*4, token_size)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X.shape = (batch_size, seq_len, input_size)\n",
    "        # q = self.Q(x) # (batch_size, seq_len, hidden_size)\n",
    "        # k = self.K(x) # (batch_size, seq_len, hidden_size)\n",
    "        # v = self.V(x) # (batch_size, seq_len, hidden_size)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = torch.chunk( qkv, 3, dim=-1)\n",
    "        x ,_= self.attn(q, k, v)\n",
    "        x = self.proj_h(x)\n",
    "        x =self.proj_out(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Bert(nn.Module):\n",
    "    hid = 768\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.bert_blocks = nn.ModuleList(\n",
    "            [BERTBlock(input_size) for _ in range(12)]\n",
    "        )\n",
    "        self.final = nn.Linear(1, self.hid)\n",
    "        self.out = nn.Linear(self.hid, output_size)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.bert_blocks:\n",
    "            x = block(x)\n",
    "        # x.shape = (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 2304])\n",
      "torch.Size([1, 10, 768]) torch.Size([1, 10, 768]) torch.Size([1, 10, 768])\n",
      "torch.Size([1, 10, 768]) torch.Size([1, 10, 768]) torch.Size([1, 10, 768])\n",
      "tensor([0.]) tensor([0.]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rand_qkv = torch.rand(1, 10, 768 * 3)\n",
    "print(rand_qkv.shape)\n",
    "q, k, v = torch.chunk(rand_qkv, 3, dim=-1)\n",
    "q1, k1, v1 = torch.split(rand_qkv, 768, dim=-1)\n",
    "print(q.shape, k.shape, v.shape)\n",
    "print(q1.shape, k1.shape, v1.shape)\n",
    "\n",
    "l2_norm = lambda x: torch.sqrt(torch.sum(x**2, dim=(-1, -2)))\n",
    "print(l2_norm(q - q1), l2_norm(k - k1), l2_norm(v - v1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
